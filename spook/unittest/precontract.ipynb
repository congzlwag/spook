{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18dca841-ea0c-4a13-a278-b3dde608f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42937286-1429-456f-b669-4d28c3eb57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contraction_utils import adaptive_contraction, shots_asarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8b444-6f65-418c-8e8e-b9e78c39eb53",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Precontraction is a great efficiency booster for covariance and spooktroscopy.\n",
    "In principle there's no absolute upper bound to number of shots, because a last resort is looping through the shots to accumulate, but that's the least efficient way.\n",
    "When it comes to memory, there are (at least) three possible limitations. From small to large these limitations are:\n",
    "\n",
    "1. Contracting with `numpy.dot` or `@`\n",
    "2. Instantiating `numpy.ndarray`s that includes all the shots\n",
    "3. Instantiating a list or dictionary that includes all the shots\n",
    "\n",
    "2 is a tighter limitation than 3 because `numpy.array` **allocates contiguous memory** to facilitate efficient operations, while the arrays in a list or a dict is not necessarily contiguous in memory.\n",
    "\n",
    "Therefore we want this contraction function to:\n",
    "1. Accept two tensors and contract over the 0th iterable \"axis\"\n",
    "2. Accept tensors in three types: `numpy.ndarray`, `list` and `dict`\n",
    "3. Accept optional slicing on each single-shot array.\n",
    "4. In addition, for backward compatibility, accept `list` or `dict` of sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3946277-d27d-42a9-96f2-ae6160506e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A Dummy dataset in three formats\n",
    "np.random.seed(802) # This notebook is created on Aug02\n",
    "Nshot = 5000\n",
    "\n",
    "# np.array \n",
    "arrdum = {}\n",
    "arrdum['A'] = np.random.randn(Nshot, 20)\n",
    "arrdum['B'] = np.random.randn(Nshot, 100)\n",
    "arrdum['C'] = np.random.randn(Nshot, 100,100)\n",
    "arrdum['C'][arrdum['C']<0.8] = 0  # This sparsifies the (100,100) arrays to ~20% survival rate\n",
    "\n",
    "# list\n",
    "lstdum = {k: [ar for ar in arrdum[k]] for k in arrdum.keys()}\n",
    "lstdum['C'] = [sps.coo_matrix(ar) for ar in lstdum['C']]\n",
    "\n",
    "# dictionary\n",
    "dctdum = {k: {i:ar for i,ar in enumerate(lstdum[k])} for k in lstdum.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358ddb45-0372-49ea-b171-126a0025bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "slca = np.s_[10:14:2]\n",
    "slcb = (30,50)\n",
    "slcv = np.s_[:200, :200]\n",
    "selecti = range(10,15)\n",
    "selectk = list(range(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99d83d-b460-4234-9f8a-7c4e743eab9c",
   "metadata": {},
   "source": [
    "### Sanity checks on `adaptive_contraction`\n",
    "The most common situation is: `A, B` are just two stacked-up matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b65008-d6bb-45b6-970a-166003f6f6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk up by 2000 shots\n",
      "True (20, 100)\n"
     ]
    }
   ],
   "source": [
    "res = adaptive_contraction(arrdum['A'], arrdum['B'])\n",
    "check = arrdum['A'].T @ arrdum['B']\n",
    "print(np.allclose(res, check), res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bf64a-dc38-4fb1-be0c-453d2d802015",
   "metadata": {},
   "source": [
    "The single shot spectra can be sliced with `a_slice, b_slice`. \n",
    "This may not seem helpful at small scale, but when it comes to memory limitations, cropping into the ROI **prior to** contraction matters, and that's what `adaptive_contraction` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d81bb9-d106-4ce3-9fe0-1465c520cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk up by 2000 shots\n",
      "True (2, 100)\n"
     ]
    }
   ],
   "source": [
    "res = adaptive_contraction(arrdum['A'], arrdum['B'], a_slice=slca)\n",
    "check = arrdum['A'][:,slca].T @ arrdum['B']\n",
    "print(np.allclose(res, check), res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37ff34-cb17-464b-83c8-8513fcef1232",
   "metadata": {},
   "source": [
    "Slicing `numpy.ndarray` is really convenient, but what if one of the tensors is too big to be stacked up into an array? This is a common situation when dealing with 5e4-shot 500x500 VMI images. In that case, slicing with `a_slice, b_slice` becomes more useful. Plus, accepting list/dictionary of sparse matrices also becomes useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8076d573-2cd7-437e-a737-c92625f61cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk up by 2000 shots\n",
      "True (20, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "tmp = np.tensordot(arrdum['A'], arrdum['C'][:,:200,:200], axes=(0,0))\n",
    "res = adaptive_contraction(dctdum['A'], dctdum['C'], b_slice=slcv, keep_dims=True)\n",
    "print(np.allclose(res, tmp), res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811ff18-6441-4f3a-bb26-627ae0e48491",
   "metadata": {},
   "source": [
    "Plus with `keep_dims=True`, `adaptive_contraction` can act so much like `np.tensordot(A, B, axes=(0,0))`. By default `keep_dims=False`, so the outcome is just the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1e80e4-83f6-40c4-a002-9a65d002adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk up by 2000 shots\n",
      "True (2, 10000)\n"
     ]
    }
   ],
   "source": [
    "tmp = np.tensordot(arrdum['A'][:,slca], arrdum['C'][:,:200,:200], axes=(0,0))\n",
    "res = adaptive_contraction(lstdum['A'], lstdum['C'], a_slice=slca, b_slice=slcv)\n",
    "print(np.allclose(res, tmp.reshape(2,-1)), res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157383d-3da1-45bc-bb90-5dade128f1f4",
   "metadata": {},
   "source": [
    "### Unit Tests for `shots_asarray`\n",
    "Although it's a critical subroutine for `adaptive_contraction`, it's not relevant to the usage of that function. Just ignore it if `adaptive_contraction` works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ac6c1df-7562-4640-8ab1-1fa439f64249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "arr = arrdum['A']\n",
    "print(np.allclose(shots_asarray(arr, None, None), (arr)))\n",
    "print(np.allclose(shots_asarray(arr, selecti, None), (arr[10:15])))\n",
    "print(np.allclose(shots_asarray(arr, None, slca), (arr[:,10:14:2])))\n",
    "print(np.allclose(shots_asarray(arr, selecti, slca), (arr[10:15,10:14:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4632d2a0-b5b9-4501-b526-ff13c9729841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "lst,arr = lstdum['B'], arrdum['B']\n",
    "print(np.allclose(shots_asarray(lst, None, None), (arr)))\n",
    "print(np.allclose(shots_asarray(lst, selecti, None), (arr[10:15])))\n",
    "print(np.allclose(shots_asarray(lst, None, slcb), (arr[:,3:5,4:6])))\n",
    "print(np.allclose(shots_asarray(lst, selecti, slcb), (arr[10:15,3:5,4:6])))\n",
    "print(np.allclose(shots_asarray(lst, selecti, slcv), (arr[10:15,:,4:6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ae4459e-2cf6-42cb-a312-fdb7b05f98eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dct,arr = dctdum['C'], arrdum['C']\n",
    "print(np.allclose(shots_asarray(dct, None, None), (arr)))\n",
    "print(np.allclose(shots_asarray(dct, selectk, None), (arr[10:15])))\n",
    "print(np.allclose(shots_asarray(dct, None, slcb), (arr[:,3:5,4:6])))\n",
    "print(np.allclose(shots_asarray(dct, selectk, slcb), (arr[10:15,3:5,4:6])))\n",
    "print(np.allclose(shots_asarray(dct, selectk, slcv), (arr[10:15,:,4:6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609a163-f913-4f65-ae03-085204d4c62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
